#!/usr/bin/env python3
"""
TÃ¼m veri kaynaklarÄ±nÄ± birleÅŸtir ve normalize et
"""

import json
import pandas as pd
from pathlib import Path
from unidecode import unidecode
import pycountry

RAW_DIR = Path(__file__).parent.parent / "data" / "raw"
PROCESSED_DIR = Path(__file__).parent.parent / "data" / "processed"
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

def normalize_country_name(country_name):
    """Ãœlke isimlerini standardize et (ISO 3166)"""

    if not country_name or pd.isna(country_name):
        return None

    # pycountry ile eÅŸleÅŸtir
    try:
        country = pycountry.countries.search_fuzzy(country_name)[0]
        return country.alpha_3  # ISO 3166-1 alpha-3 code (USA, GBR, TUR)
    except:
        # Manuel dÃ¼zeltmeler
        manual_mapping = {
            'United States': 'USA',
            'United Kingdom': 'GBR',
            'Russia': 'RUS',
            'Turkey': 'TUR',
            'TÃ¼rkiye': 'TUR',
            'South Korea': 'KOR',
            'North Korea': 'PRK',
        }
        return manual_mapping.get(country_name, None)

def normalize_name(name):
    """Ä°simleri normalize et (lowercase, unicode)"""

    if not name or pd.isna(name):
        return None

    # KÃ¼Ã§Ã¼k harfe Ã§evir, unicode normalize
    normalized = unidecode(str(name).strip().lower())
    return normalized if normalized else None

def load_wikipedia_data():
    """Wikipedia/Wikidata verilerini yÃ¼kle"""

    print("ðŸ“š Wikipedia verileri yÃ¼kleniyor...")

    file_path = RAW_DIR / "wikipedia" / "wikidata_persons.json"

    if not file_path.exists():
        print(f"  âš ï¸  Dosya bulunamadÄ±: {file_path}")
        return pd.DataFrame()

    with open(file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)

    df = pd.DataFrame(data)

    # Parse names
    df['first_name'] = df['name'].str.split().str[0]
    df['last_name'] = df['name'].str.split().str[-1]

    # Normalize
    df['first_name_norm'] = df['first_name'].apply(normalize_name)
    df['last_name_norm'] = df['last_name'].apply(normalize_name)
    df['country_code'] = df['country'].apply(normalize_country_name)

    # Columns
    result = df[['first_name_norm', 'last_name_norm', 'country_code', 'ethnicity']].copy()
    result['source'] = 'wikipedia'

    print(f"  âœ“ {len(result)} kayÄ±t yÃ¼klendi")
    return result

def load_olympics_data():
    """Olympics verilerini yÃ¼kle"""

    print("ðŸ… Olympics verileri yÃ¼kleniyor...")

    file_path = RAW_DIR / "olympics" / "olympics_names.csv"

    if not file_path.exists():
        print(f"  âš ï¸  Dosya bulunamadÄ±: {file_path}")
        return pd.DataFrame()

    df = pd.read_csv(file_path)

    # Normalize
    df['first_name_norm'] = df['first_name'].apply(normalize_name)
    df['last_name_norm'] = df['last_name'].apply(normalize_name)
    df['country_code'] = df['region'].apply(normalize_country_name)

    # Columns
    result = df[['first_name_norm', 'last_name_norm', 'country_code']].copy()
    result['ethnicity'] = None
    result['source'] = 'olympics'

    print(f"  âœ“ {len(result)} kayÄ±t yÃ¼klendi")
    return result

def load_phone_directories_data():
    """Telefon rehberi verilerini yÃ¼kle"""

    print("ðŸ“ž Telefon rehberi verileri yÃ¼kleniyor...")

    phone_dir = RAW_DIR / "phone_directories"

    if not phone_dir.exists():
        print(f"  âš ï¸  KlasÃ¶r bulunamadÄ±: {phone_dir}")
        return pd.DataFrame()

    all_data = []

    # TÃ¼m .txt dosyalarÄ±nÄ± oku
    for file_path in phone_dir.glob("*.txt"):
        # Dosya adÄ±ndan Ã¼lke kodu Ã§Ä±kar (Ã¶rn: us_surnames.txt -> US)
        filename = file_path.stem
        parts = filename.split('_')

        if len(parts) >= 1:
            country_code = parts[0].upper()

            # Ä°sim tipini belirle
            is_surname = 'surname' in filename.lower()

            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                for line in f:
                    name = line.strip()
                    if name:
                        normalized = normalize_name(name)

                        all_data.append({
                            'first_name_norm': None if is_surname else normalized,
                            'last_name_norm': normalized if is_surname else None,
                            'country_code': country_code if len(country_code) <= 3 else None,
                            'ethnicity': None,
                            'source': 'phone_directory'
                        })

    df = pd.DataFrame(all_data)
    print(f"  âœ“ {len(df)} kayÄ±t yÃ¼klendi")
    return df

def merge_all_sources():
    """TÃ¼m kaynaklarÄ± birleÅŸtir"""

    print("\nðŸ”— TÃ¼m kaynaklar birleÅŸtiriliyor...\n")

    # TÃ¼m kaynaklarÄ± yÃ¼kle
    wiki_df = load_wikipedia_data()
    olympics_df = load_olympics_data()
    phone_df = load_phone_directories_data()

    # BirleÅŸtir
    all_data = pd.concat([wiki_df, olympics_df, phone_df], ignore_index=True)

    # Temizlik
    all_data = all_data.dropna(subset=['first_name_norm', 'country_code'], how='all')
    all_data = all_data[all_data['country_code'].notna()]

    # Duplicate kontrolÃ¼ (aynÄ± isim-Ã¼lke Ã§ifti)
    print(f"\nðŸ“Š Toplam kayÄ±t: {len(all_data)}")
    print(f"ðŸ“Š Benzersiz first name-country: {all_data[['first_name_norm', 'country_code']].drop_duplicates().shape[0]}")
    print(f"ðŸ“Š Benzersiz last name-country: {all_data[['last_name_norm', 'country_code']].drop_duplicates().shape[0]}")

    # Kaydet
    output_file = PROCESSED_DIR / "merged_names.csv"
    all_data.to_csv(output_file, index=False, encoding='utf-8')

    print(f"\nâœ… BirleÅŸtirilmiÅŸ veri kaydedildi: {output_file}")

    # Ä°statistikler
    print("\nðŸ“Š Kaynak daÄŸÄ±lÄ±mÄ±:")
    print(all_data['source'].value_counts())

    print("\nðŸ“Š Ãœlke daÄŸÄ±lÄ±mÄ± (top 30):")
    print(all_data['country_code'].value_counts().head(30))

    return all_data

if __name__ == "__main__":
    merge_all_sources()
